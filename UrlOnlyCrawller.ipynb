{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 로직\n",
    " #### 1. 검색하고 싶은 키워드(해시태그)를 리스트에 넣는다\n",
    " #### 2. 해당 키워드에 대한 페이지로 들어가게된다\n",
    " #### 3. 스크롤을 내리면서 각 게시물의 URL 크롤링\n",
    " #### 4. 수집한 URL을 CSV파일로 만듬\n",
    " #### 5. 리스트에 있는 모든 키워드에 대해 1~4 반복\n",
    " \n",
    " ##### cf) URL 크롤러와 TAG 크롤러를 구분했다 \n",
    " #####  => 수집된 URL에 대한  TAG 크롤링은 약 5~6개의 크롤러로 나눠서 돌리는게 훨씬 빠르다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrlCrawller:\n",
    "    last = \"\" # 이전에 크롤링된 URL 리스트의 마지막 URL -> 중복을 방지하기 위함\n",
    "    count = 0 # 현재까지 크롤링된 URL 개수\n",
    "    zero_repeat = 0\n",
    "    \n",
    "    #크롤러 객체 초기화\n",
    "    def __init__(self, chromedriver_path, keyword, url_csv_filepath, target_num, SCROLL_PAUSE_TIME = 3):\n",
    "        self.keyword = keyword\n",
    "        self.target_url = \"https://www.instagram.com/explore/tags/\"+keyword\n",
    "        self.driver = webdriver.Chrome(chromedriver_path)\n",
    "        self.url_csv_filepath = url_csv_filepath\n",
    "        self.target_num = target_num\n",
    "        self.SCROLL_PAUSE_TIME = SCROLL_PAUSE_TIME\n",
    "        UrlCrawller.last = \"\"\n",
    "        UrlCrawller.count=0\n",
    "        UrlCrawller.zero_repeat = 0\n",
    "        \n",
    "    #크롤러 객체 준비\n",
    "    def crawlling_ready(self):\n",
    "        #기존에 파일 있는지 유무 확인\n",
    "        if(not os.path.exists(self.url_csv_filepath)):\n",
    "            print('파일이 없으므로 생성 후 크롤링을 시작합니다')\n",
    "        \n",
    "        else:\n",
    "            print('파일이 있으므로 기존의 파일에 덧붙이겠습니다.')\n",
    "        \n",
    "        self.driver.implicitly_wait(3) \n",
    "        self.driver.get(self.target_url)\n",
    "        \n",
    "        #포스팅 개수 추출, 문자열\n",
    "        posting_gatsu_str = self.driver.find_element_by_xpath('.//span[@class=\"g47SY \"]').text \n",
    "        \n",
    "        #문자열로 추출된 개수를 정수형으로 변환\n",
    "        posting_gatsu = int(posting_gatsu_str.replace(\",\", \"\"))\n",
    "        \n",
    "        #사용자가 원하는 크롤링 개수보다 전체 게시물이 적을 경우\n",
    "        if self.target_num > posting_gatsu:\n",
    "            self.target_num = posting_gatsu\n",
    "        \n",
    "        print(\"--------------태그 : %s -------------------\" %self.keyword)\n",
    "        print(\"총 게시물 : \", posting_gatsu)\n",
    "        self.url_crawlling()\n",
    "    \n",
    "    \n",
    "    # 새롭게 크롤링 된 URL 리스트에 이전에 수집된 URL 리스트 목록에 있는 일부 요소들이 중복되어 들어가있다. \n",
    "    # 따라서 URL을 긁어올 떄 마다 해당 리스트의 마지막 URL을 저장해놓고 \n",
    "    # 새롭게 크롤링 된 URL 리스트에서 이전 리스트의 마지막 URL 다음부터 저장할 수 있도록 한다\n",
    "    def url_crawlling(self):\n",
    "        UrlCrawller.last = \"\"\n",
    "        while True:\n",
    "            source = self.driver.page_source # 바이트코드 type으로 소스를 읽는다.\n",
    "            soup = BeautifulSoup(source, 'lxml') # 파싱할 문서를 BeautifulSoup 클래스의 생성자에 넘긴다\n",
    "            item = soup.findAll('a', {\"href\": re.compile('^/p/')}) # 해당 영역 html 형식 -> <a href=\"/p/Bk_Jqc5gNYC/?tagged=sydney\">\n",
    "            item_len = len(item)\n",
    "            position = 0 # 이전 리스트와 중복되지 않는 부분부터 시작하기 위한 변수\n",
    "            new_item = [] # 이전 리스트와 중복되지 않는 부분부터 시작되는 새로운 리스트\n",
    "            \n",
    "            # 두 번째 크롤링부터는 이전 리스트의 마지막 url 값이 공백이 아니므로 이를 처리하기 위함\n",
    "            if UrlCrawller.last!=\"\":\n",
    "                for i in item:\n",
    "                    if UrlCrawller.last!=i.attrs['href']:\n",
    "                        position+=1 \n",
    "                        continue\n",
    "                    \n",
    "                    else:\n",
    "                        position+=1\n",
    "                        break\n",
    "            \n",
    "            # 이전 리스트와 중복되지 않는 부분부터 시작되는 새로운 리스트를 만든다\n",
    "            for i in range(position,item_len):\n",
    "                current_href = item[i].attrs['href'] # 해당 영역 html 형식 -> <a href=\"/p/Bk_Jqc5gNYC/?tagged=sydney\">\n",
    "                new_item.append(current_href)\n",
    "                \n",
    "                if i==item_len-1:\n",
    "                    UrlCrawller.last = item[i].attrs['href']\n",
    "           \n",
    "            #새로운 리스트 길이\n",
    "            new_item_len = len(new_item)\n",
    "              \n",
    "            #새로운 리스트가 이전 리스트와 동일한 경우\n",
    "            if new_item_len==0:\n",
    "                UrlCrawller.zero_repeat+=1\n",
    "                time.sleep(1)\n",
    "                print(\"응답 없음 : \",UrlCrawller.zero_repeat)\n",
    "                \n",
    "                if UrlCrawller.zero_repeat==10:\n",
    "                    # 새로 고침한뒤 다시 시작\n",
    "                    # 마지막 url 있는데까지 스크롤 내리기\n",
    "                    return self.refresh_start_over(UrlCrawller.count)\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                UrlCrawller.zero_repeat = 0\n",
    "                #url 을 저장하기 위한 csv 파일작성     \n",
    "                self.write_csv_url(new_item)\n",
    "                \n",
    "            \n",
    "            # url 크롤링 몇 개까지 완료되었는지\n",
    "            UrlCrawller.count+= new_item_len\n",
    "            print(\"(%d / %d)\" %(UrlCrawller.count, self.target_num))\n",
    "            \n",
    "            if(UrlCrawller.count >= self.target_num):\n",
    "                #return print(\"%s URL 크롤링 완료!\\n\\n\" %self.keyword)\n",
    "                print(\"%s URL 크롤링 완료!\\n\\n\" %self.keyword)\n",
    "                self.driver.close()\n",
    "                break\n",
    "             \n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(self.SCROLL_PAUSE_TIME)\n",
    "     \n",
    "    \n",
    "    # 새로 고침한뒤 다시 시작\n",
    "    # 수집된 개수만큼 움직였을 떄 다시 수집\n",
    "    def refresh_start_over(self,success_cnt):\n",
    "        print(\"새로고침!\")\n",
    "        print(\"success_cnt : \",success_cnt)\n",
    "        UrlCrawller.zero_repeat = 0\n",
    "        self.driver.refresh()\n",
    "        time.sleep(7)\n",
    "        \n",
    "        re_count = 0\n",
    "        UrlCrawller.last=\"\"\n",
    "        \n",
    "        while True:\n",
    "            source = self.driver.page_source # 바이트코드 type으로 소스를 읽는다.\n",
    "            soup = BeautifulSoup(source, 'lxml') # 파싱할 문서를 BeautifulSoup 클래스의 생성자에 넘긴다\n",
    "            item = soup.findAll('a', {\"href\": re.compile('^/p/')}) # 해당 영역 html 형식 -> <a href=\"/p/Bk_Jqc5gNYC/?tagged=sydney\">\n",
    "            item_len = len(item)\n",
    "            position = 0 # 이전 리스트와 중복되지 않는 부분부터 시작하기 위한 변수\n",
    "            new_item = [] # 이전 리스트와 중복되지 않는 부분부터 시작되는 새로운 리스트\n",
    "            \n",
    "            if UrlCrawller.last!=\"\":\n",
    "                for i in item:\n",
    "                    if UrlCrawller.last!=i.attrs['href']:\n",
    "                        position+=1 \n",
    "                        continue\n",
    "                    \n",
    "                    else:\n",
    "                        position+=1\n",
    "                        break\n",
    "            \n",
    "            # 이전 리스트와 중복되지 않는 부분부터 시작되는 새로운 리스트를 만든다\n",
    "            for i in range(position,item_len):\n",
    "                current_href = item[i].attrs['href'] # 해당 영역 html 형식 -> <a href=\"/p/Bk_Jqc5gNYC/?tagged=sydney\">\n",
    "                new_item.append(current_href)\n",
    "                \n",
    "                if i==item_len-1:\n",
    "                    UrlCrawller.last = item[i].attrs['href']\n",
    "           \n",
    "            #새로운 리스트 길이\n",
    "            new_item_len = len(new_item)\n",
    "            \n",
    "            #새로운 리스트가 이전 리스트와 동일한 경우\n",
    "            if new_item_len==0:\n",
    "                UrlCrawller.zero_repeat+=1\n",
    "                time.sleep(1)\n",
    "                \n",
    "                if UrlCrawller.zero_repeat==10:\n",
    "                    return self.refresh_start_over(success_cnt)\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                UrlCrawller.zero_repeat = 0\n",
    "                re_count+= new_item_len\n",
    "                print(\"re count : \", re_count)\n",
    "                if re_count >= success_cnt:\n",
    "                    return self.url_crawlling()\n",
    "                \n",
    "            \n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(self.SCROLL_PAUSE_TIME)\n",
    "\n",
    "\n",
    "    #url 을 저장하기 위한 csv 파일을 만든다     \n",
    "    def write_csv_url(self,list):\n",
    "        with codecs.open(self.url_csv_filepath,\"a\",\"utf-8\") as fp:\n",
    "            writer=csv.writer(fp,delimiter=\",\")\n",
    "            writer.writerow(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(keyword):\n",
    "    url_csv_filepath = 'C:/Users/acorn/weather_bigdata/original_url/url_'+keyword+'.csv'\n",
    "    chromedriver_path = 'C:/Users/acorn/Desktop/weather/chromedriver_win32/chromedriver'\n",
    "    target_url = \"https://www.instagram.com/explore/tags/\"+keyword\n",
    "    target_num = 100000 # 크롤링할 게시물 개수\n",
    "    crawller = UrlCrawller(chromedriver_path, keyword, url_csv_filepath, target_num)\n",
    "    crawller.crawlling_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링할 태그를 입력하세요 : 제주도\n",
      "파일이 있으므로 기존의 파일에 덧붙이겠습니다.\n",
      "--------------태그 : 제주도 -------------------\n",
      "총 게시물 :  8630535\n",
      "(21 / 100000)\n",
      "(45 / 100000)\n",
      "(57 / 100000)\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=68.0.3440.106)\n  (Driver info: chromedriver=2.40.565498 (ea082db3280dd6843ebfb08a625e3eb905c4f5ab),platform=Windows NT 6.1.7601 SP1 x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6d9ebc582671>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"크롤링할 태그를 입력하세요 : \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-d88479eb126d>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(keyword)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtarget_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100000\u001b[0m \u001b[1;31m# 크롤링할 게시물 개수\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcrawller\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUrlCrawller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchromedriver_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl_csv_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mcrawller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawlling_ready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-bd2a8e30a21d>\u001b[0m in \u001b[0;36mcrawlling_ready\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--------------태그 : %s -------------------\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"총 게시물 : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposting_gatsu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl_crawlling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-bd2a8e30a21d>\u001b[0m in \u001b[0;36murl_crawlling\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mUrlCrawller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m \u001b[1;31m# 바이트코드 type으로 소스를 읽는다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 파싱할 문서를 BeautifulSoup 클래스의 생성자에 넘긴다\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mitem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"href\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'^/p/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 해당 영역 html 형식 -> <a href=\"/p/Bk_Jqc5gNYC/?tagged=sydney\">\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mpage_source\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    670\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \"\"\"\n\u001b[1;32m--> 672\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET_PAGE_SOURCE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    316\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=68.0.3440.106)\n  (Driver info: chromedriver=2.40.565498 (ea082db3280dd6843ebfb08a625e3eb905c4f5ab),platform=Windows NT 6.1.7601 SP1 x86_64)\n"
     ]
    }
   ],
   "source": [
    "keyword = input(\"크롤링할 태그를 입력하세요 : \")\n",
    "main(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
